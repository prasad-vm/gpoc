main.py

""" Main Module """
import os
import sys
import json
import subprocess
import logging
from concurrent.futures import ThreadPoolExecutor
from dotenv import load_dotenv

# Load env variables
load_dotenv()

# logger setup
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Path variables
SPECS_DIR = os.getenv("SPECS_DIR")
AUTOMATION_SCRIPT = os.getenv("AUTOMATION_SCRIPT")


class SpecProcessor:
    """SpecProcessor Class"""
    def __init__(
            self, specs_dir=SPECS_DIR, automation_script=AUTOMATION_SCRIPT):
        self.specs_dir = specs_dir
        self.automation_script = automation_script

    def process_json_specs(self, json_file, spec_name):
        """Process JSON specs and call automation script"""
        json_spec_path = os.path.join(self.specs_dir, json_file)
        self.process_single_spec(json_spec_path, spec_name)

    def process_single_spec(self, json_spec_path, spec_name):
        """Process single JSON spec file."""
        try:
            with open(json_spec_path, "r", encoding="utf-8") as file:
                json_spec = json.load(file)
                logger.info(json_spec)

            process = subprocess.run(
                ["python", self.automation_script, json_spec_path, spec_name],
                check=True,
            )
            if process.returncode != 0:
                logger.error(
                    "Error running automation script: %s", process.returncode)
        except FileNotFoundError:
            logger.error("File not found: %s", json_spec_path)
        except json.JSONDecodeError:
            logger.error("Error decoding JSON file: %s", json_spec_path)
        except subprocess.CalledProcessError as e:
            logger.error("Error running automation script: %s", e)


def main():
    """Main function for main script"""
    if len(sys.argv) != 3:
        logger.error("Usage: python main.py <spec_name> <json_input_file>")
        sys.exit(1)

    spec_name = sys.argv[1]
    json_file = sys.argv[2]

    if not os.path.exists(SPECS_DIR):
        logger.error("Specs directory does not exist: %s", SPECS_DIR)
        return

    processor = SpecProcessor()
    with ThreadPoolExecutor() as executor:
        executor.submit(processor.process_json_specs, json_file, spec_name)


if __name__ == "__main__":
    main()

automain.py

""" Automation Module """
import os
import sys
import json
import shutil
import logging
import yaml
from dotenv import load_dotenv


# Load env variables
load_dotenv()

# logger setup
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# path env variables
SPEC_DIR = os.getenv('SPEC_DIR')
IAC_DIR = os.getenv('IAC_DIR')


class Automation:
    """AUtomation Class"""
    def __init__(self, spec_name, json_spec_path):
        self.spec_name = spec_name
        self.json_spec_path = json_spec_path

    def load_template(self, env):
        """Load YAML template based on the spec name and env"""
        try:
            template_file = f"{env}.yaml"
            template_path = os.path.join(
                SPEC_DIR, "specifications", self.spec_name, template_file
            )
            if not os.path.exists(template_path):
                raise FileNotFoundError(f"Template not found: {template_path}")
            with open(template_path, "r", encoding='utf-8') as file:
                template = yaml.safe_load(file)
            return template
        except FileNotFoundError as e:
            logger.error("%s", e)
            raise
        except yaml.YAMLError as e:
            logger.error("Error loading YAML template: %s", template_path)
            raise ValueError(f"Error loading YAML template: %s, {template_path}") from e

    def merge_spec_with_template(self, spec, template):
        """Merge JSON spec with YAML template."""
        try:
            template.update(spec)
            return template
        except KeyError as e:
            logger.error("Missing required spec key: %s", e)
            raise

    def save_to_iac_repo(self, merged_spec, output_filename, cluster_name):
        """Save merged spec to the IaC repository"""
        output_path = os.path.join(
            IAC_DIR, "clusters", cluster_name, "definitions", output_filename
        )
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        with open(output_path, "w", encoding='utf-8') as file:
            yaml.dump(merged_spec, file)
        logger.info("Cluster definition saved to: %s", output_path)

    def copy_configuration_files(self, cluster_name):
        """Copy config files to IaC repository."""
        config_source_dir = os.path.join(
            SPEC_DIR, "configurations", self.spec_name)
        config_target_dir = os.path.join(
            IAC_DIR, "clusters", cluster_name, "configurations"
        )

        if not os.path.exists(config_source_dir):
            logger.warning(
                "No configuration files found for spec: %s", self.spec_name)
            return

        os.makedirs(config_target_dir, exist_ok=True)

        for filename in os.listdir(config_source_dir):
            full_file_name = os.path.join(config_source_dir, filename)
            if os.path.isfile(full_file_name):
                shutil.copy(full_file_name, config_target_dir)
                logger.info("Copied %s to %s", filename, config_target_dir)

    def validate_spec(self, spec):
        """Validate required keys are present in the spec"""
        required_keys = [
            "clusterName",
            "env",
            "workerNodeCount",
            "controlPlaneCount",
            "region",
            "spec",
            "kubernetesVersion",
            "networkPolicy",
            "storageClass",
        ]
        for key in required_keys:
            if key not in spec:
                raise KeyError(f"Missing required key in spec: {key}")

    def run(self):
        """Run the automation process"""
        try:
            with open(self.json_spec_path, "r", encoding='utf-8') as file:
                json_spec = json.load(file)

            self.validate_spec(json_spec)

            env = json_spec.get("env", "dev")
            cluster_name = json_spec.get("clusterName")
            template = self.load_template(env)
            merged_spec = self.merge_spec_with_template(json_spec, template)

            output_filename = f"{cluster_name}-cluster-definition.yaml"
            self.save_to_iac_repo(merged_spec, output_filename, cluster_name)
            self.copy_configuration_files(cluster_name)
        except FileNotFoundError as e:
            logger.error(e)
        except json.JSONDecodeError:
            logger.error("Error decoding JSON file: %s", self.json_spec_path)
        except KeyError as e:
            logger.error(e)
        except ValueError as e:
            logger.error(e)


def main():
    """Main function to run the automation script"""
    if len(sys.argv) != 3:
        logger.error(
            "Usage: python automation.py <json_spec_path> <spec_name>")
        sys.exit(1)

    json_spec_path = sys.argv[1]
    spec_name = sys.argv[2]

    automation = Automation(spec_name, json_spec_path)
    automation.run()


if __name__ == "__main__":
    main()

input1.json

{
  "clusterName": "dev-cluster",
  "env": "dev",
  "workerNodeCount": 6,
  "controlPlaneCount": 1,
  "region": "eastus",
  "spec": "shared",
  "kubernetesVersion": "1.20.7",
  "networkPolicy": "calico",
  "storageClass": "default"
}

shared-v01.json
{
    "specname": "shared",
    "version": "0.1",
    "targetProvider": "aks",
    "cluster": {
      "clusterDefinition": {
        "template": "general-medium.yaml"
      },
      "configuration": {
        "helmChartsInRegistry": [
          {
            "sources": "JFrog",
            "charts": {
              "chart_path_from_jfffrog_registry": ""
            }
          }
        ],
        "ansibleScripts": ["connfig-v0.1.yaml"],
        "shScripts": []
      },
      "qualitycheck": {
        "testSuite": "all-tests"
      }
    }
  }

general-medium-v0.1.yaml
apiVersion: v1
kind: Cluster
metadata:
  name: <clusterName>
  labels:
    env: <env>
spec:
  controlPlane:
    count: <controlPlaneCount>
    nodeType: <nodeType>
  workerNodes:
    count: <workerNodeCount>
    nodeType: <nodeType>
  network:
    policy: <networkPolicy>
  storage:
    class: <storageClass>
  version:
    kubernetesVersion: <kubernetesVersion>
  region: <region>

